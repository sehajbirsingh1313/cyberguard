{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yg9I30dpqXTN"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (uploaded to Colab locally)\n",
        "train_path = '/content/train.csv'  # Path to your uploaded train.csv file\n",
        "test_path = '/content/test.csv'   # Path to your uploaded test.csv file\n",
        "\n",
        "# Load train and test datasets\n",
        "train_data = pd.read_csv(train_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "VYgEywBZx8dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the training data\n",
        "print(\"Training Data Overview:\")\n",
        "print(train_data.head())\n",
        "\n",
        "# Handle missing values\n",
        "train_data['crimeaditionalinfo'] = train_data['crimeaditionalinfo'].fillna(\"\")\n",
        "train_data['sub_category'] = train_data['sub_category'].fillna(\"Unknown\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4kx74_93yEZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine rare classes in 'sub_category'\n",
        "min_class_threshold = 5  # Minimum number of samples per class\n",
        "class_counts = train_data['sub_category'].value_counts()\n",
        "rare_classes = class_counts[class_counts < min_class_threshold].index\n",
        "train_data['sub_category'] = train_data['sub_category'].replace(rare_classes, 'Other')\n",
        "\n",
        "# Define Features and Labels\n",
        "X = train_data['crimeaditionalinfo']\n",
        "y = train_data['sub_category']\n",
        "\n"
      ],
      "metadata": {
        "id": "7lV_NZBvyK4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "v20AH5ttyPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer and Dataset Preparation\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = CustomDataset(X_train, y_train, tokenizer)\n",
        "test_dataset = CustomDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "# Load Pretrained BERT Model for Sequence Classification\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(label_encoder.classes_)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "lFopjQR7yXEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Trainer Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=lambda p: {\n",
        "        'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
        "        'precision': precision_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted'),\n",
        "        'recall': recall_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted'),\n",
        "        'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted')\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "FdpNlQl3yYQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the Model\n",
        "results = trainer.evaluate()\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"Accuracy: {results['eval_accuracy']:.4f}\")\n",
        "print(f\"Precision: {results['eval_precision']:.4f}\")\n",
        "print(f\"Recall: {results['eval_recall']:.4f}\")\n",
        "print(f\"F1-Score: {results['eval_f1']:.4f}\")\n",
        "\n",
        "# Save Model and Tokenizer\n",
        "model.save_pretrained('/content/bert_model')\n",
        "tokenizer.save_pretrained('/content/bert_model')\n",
        "\n",
        "print(\"Model and tokenizer saved to /content/bert_model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a4659gwyyhvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Predict\n",
        "def predict_complaint(text, model, tokenizer, label_encoder, max_len=128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=max_len,\n",
        "        add_special_tokens=True,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_mask = encoding['attention_mask']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return label_encoder.inverse_transform([prediction])[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "8vmf5V5tylMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Prediction\n",
        "sample_complaint = \"I applied for a loan online but the lender is blackmailing me for more money.\"\n",
        "loaded_model = BertForSequenceClassification.from_pretrained('/content/bert_model')\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained('/content/bert_model')\n",
        "predicted_category = predict_complaint(sample_complaint, loaded_model, loaded_tokenizer, label_encoder)\n",
        "print(f\"Predicted Sub-Category for the Complaint: {predicted_category}\")"
      ],
      "metadata": {
        "id": "0yaxZ9dMynT6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}